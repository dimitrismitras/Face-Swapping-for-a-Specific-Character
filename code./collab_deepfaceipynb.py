# -*- coding: utf-8 -*-
"""collab_deepfaceipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qtBa_CcnHP-w2hs2i4LEtVJtSnW-FRIb

#DeepFake Video Creation

In this project, a Face Swapping video is presented, featuring the transformation of a specific face in a video with another face.

The project is divided into three thematic sections for implementing Face Swapping: **1) Face Detection, 2) Face Recognition, and 3) Face Swapping**. To create and process the video, the cv2 library is used. Specifically, each video is divided into frames, where individual parts are processed, resulting in a set of modified frames. In the end, the new video with the Face Swapping of a specific character is saved.

***Face Detection***: This involves finding a face in a frame. The InsightFace library was used for this task. Among the available Face Detectors, RetinaFace was preferred as, although slower, it yields better results and provides ideal facial angle cropping compared to others (e.g., OpenCV, YOLO, etc.).

***Face Recognition***: This involves identifying a face compared to existing data. Implementation was done using the DeepFace library. Specifically, identification is made between the detected Face and images of characters from the F.R.I.E.N.D.S series, which are sorted into subfolders by their names. From this image pool, face detection is also performed, followed by a comparison of the two faces (i.e., the Face from the detected frame and the face in the image from the dataset, both extracted by the face detector), resulting in a true or false outcome. The image pool was obtained from https://www.kaggle.com/datasets/amiralikalbasi/images-of-friends-character-for-face-recognition. If the result is false, it proceeds to the next image/face until all are checked. If true, the check stops, and it proceeds to the Face Recognition stage.

***Face Swapping***: This involves changing one face with another. Again, with the help of the InsightFace library. The face extracted from the Face Detection stage and matched with a character in the Face Recognition stage is replaced with another of our choice. The process is repeated for each frame.

The project structure includes separate examples for each section (Face Detection, Face Recognition, Face Swapping), and at the end, a combination of all three is used to generate a video with the face of a specific character swapped (VIDEO SWAPPING FOR SPECIFIC CHARACTER (INSIGHT FACE & DEEP FACE)).
"""



"""#Install Libraries"""

from google.colab import drive
drive.mount('/content/drive')

!pip install onnxruntime-gpu

!pip install -U insightface

!pip install typing_extensions==4.7.1

from typing_extensions import Concatenate

"""#Face Detection (INSIGHT FACE)"""

import numpy as np
import os
import glob
import cv2
import matplotlib.pyplot as plt

import insightface
from insightface.app import FaceAnalysis
from insightface.data import get_image as ins_get_image

###STEP 1 Detect Faces
app = FaceAnalysis(name = 'buffalo_l')
app.prepare(ctx_id=0 , det_size = (640 , 640))

print('insightface' , insightface.__version__)
print('numpy' , np.__version__)



import matplotlib.image as mpimg

img = ins_get_image('t1') #friends
#img = cv2.imread('/content/drive/MyDrive/Colab Notebooks/Deepfake/friends.png')

plt.imshow(img[:,:,::-1])
plt.axis('off')
#plt.savefig('friends.png')  # Save the image to a file
plt.show()

"""##Faces Detection (Image)"""

faces = app.get(img)
rimg = app.draw_on(img, faces)
plt.figure(figsize=(10, 8))
plt.imshow(rimg[:,:,::-1])
plt.axis('off')
#plt.savefig('celebrity output.png')  # Save the image to a file
plt.show()

#plot all faces
fig, axs = plt.subplots(1, 6, figsize=(12,5))

for i, face in enumerate(faces):
  face = app.get(img)
  bbox = face['bbox']
  bbox = [int(b) for b in bbox]
  axs[i].imshow(img[bbox[1]:bbox[3], bbox[0]:bbox[2],::-1])
  axs[i].axis('off')

"""##Faces Detection (Video)"""

import cv2
import imageio
from IPython.display import HTML, display

# Specify the path to your input video
video_path_input = 'video3.mp4'

# Create a VideoCapture object
cap = cv2.VideoCapture(video_path_input)

# Create a list to store frames
frames = []

# Process frames
while (cap.isOpened()):
    ret, frame = cap.read()

    if not ret:
        break

    # Detect faces in the frame
    faces = app.get(frame)

    # Draw bounding boxes on the frame based on face detection
    result = app.draw_on(frame, faces)

    # Display the result using matplotlib (optional)
    plt.imshow(result[:, :, ::-1])  # RGB color space, [0, 255] range
    plt.pause(0.01)

    # Append the frame to the list
    frames.append(result)

    # Check for keyboard input using plt.waitforbuttonpress (optional)
    if plt.waitforbuttonpress(timeout=1e-3):
        break

# Release resources
cap.release()
plt.close('all')  # Close all matplotlib figures

# Specify the path for the output video
video_path_output = 'output_video3.mp4'

# Write frames to the output video using imageio with consistent color space and range
imageio.mimsave(video_path_output, [img[:, :, ::-1] for img in frames], fps=30, macro_block_size=None)

"""##Compare face detectors


"""

!pip install mediapipe
!pip install ultralytics
from deepface import DeepFace
import cv2
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# Load an image
img_path = "friends/Train/Rachel/rachel (14).jpg"
img = cv2.imread(img_path)

detector_backends = [
    "opencv",
    "ssd",
    "mtcnn",
    "mediapipe",
    "retinaface",
    "yunet",
    "yolov8",
]

fig, axs = plt.subplots(1, len(detector_backends) + 1, figsize=(15, 5), gridspec_kw={'width_ratios': [1, 1, 1, 1, 1, 1, 1, 1]})

# Plot the original image
axs[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
axs[0].set_title('Original Image')
axs[0].axis('off')

for i, backend in enumerate(detector_backends):
    # Perform face detection
    faces = DeepFace.extract_faces(img_path, target_size=(224, 224), detector_backend=backend)

    # Plot only the detected faces
    for face_info in faces:
        x, y, w, h = face_info["facial_area"]["x"], face_info["facial_area"]["y"], face_info["facial_area"]["w"], face_info["facial_area"]["h"]
        face = img[y:y+h, x:x+w]
        axs[i + 1].imshow(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))
        axs[i + 1].set_title(f'{backend} Detection')
        axs[i + 1].axis('off')
        break  # Break after plotting the first detected face

plt.tight_layout()
plt.show()



"""#Face Swapping"""

### Step 2. Face **Swapping**

"""##Face Swapping (Image) one person"""

#Detect the Face
mitras = cv2.imread('elon mask 2.jpg')
mitras_face = app.get(mitras)
mitras_face=mitras_face[0]
#Replace faces in friends image

res = img.copy()
for face in faces:
    res = swapper.get(res, face, mitras_face, paste_back=True)

plt.figure(figsize=(10, 8))
plt.axis('off')

plt.imshow(res[:,:,::-1])
plt.savefig('celebrity mitras.png')
plt.show()

!pip install imageio[ffmpeg]

"""##Face Swapping (Video) one person"""

import cv2
import imageio
from IPython.display import HTML, display

swapper = insightface.model_zoo.get_model('inswapper_128.onnx',
                                          download=False,
                                          download_zip=False)

#Detect the Face
mitras = cv2.imread('elon mask 2.jpg')
mitras_face = app.get(mitras)
mitras_face=mitras_face[0]
# Specify the path to your input video
video_path_input = 'video2.mp4'

# Create a VideoCapture object
cap = cv2.VideoCapture(video_path_input)

# Create a list to store frames
frames = []

c=0

# Process frames
while (cap.isOpened()):
    ret, frame = cap.read()

    c+=1
    if not ret:
        break

    # Detect faces in the frame
    faces = app.get(frame)

    # Face swapping
    result = frame.copy()
    #for face in faces:
    result = swapper.get(result, faces[0], mitras_face, paste_back=True)

    # Display the result using matplotlib (optional)
    plt.imshow(result[:, :, ::-1])  # RGB color space, [0, 255] range
    plt.pause(0.01)

    # Append the frame to the list
    frames.append(result)

    # Check for keyboard input using plt.waitforbuttonpress (optional)
    if plt.waitforbuttonpress(timeout=1e-3):
        break

# Release resources
cap.release()
plt.close('all')  # Close all matplotlib figures

# Specify the path for the output video
video_path_output = 'output multiple.mp4'

# Write frames to the output video using imageio with consistent color space and range
imageio.mimsave(video_path_output, [img[:, :, ::-1] for img in frames], fps=30, macro_block_size=None)

!pip install --upgrade matplotlib

"""#Face Recognition (DEEPFACE)

"""

!pip install --upgrade deepface

#Face Recognition
from deepface import DeepFace
import matplotlib.pyplot as plt
import cv2

"""##Faces Recognition for Multiple Faces (image)"""

from deepface import DeepFace
import os
import cv2
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# Load the image with lots of faces
img = ins_get_image('t1')

# Path to the directory containing images of different people
base_path = "friends/Train"

# Get a list of subdirectories (each subdirectory represents a person)
people_dirs = [f.path for f in os.scandir(base_path) if f.is_dir()]

# Initialize lists to store rectangles and labels
rectangles = []
labels = []

# Iterate through each person's directory
for person_dir in people_dirs:
    for img_path in os.listdir(person_dir):
        img_path = os.path.join(person_dir, img_path)

        # Load the image to verify
        img_to_verify = cv2.imread(img_path)

        # Perform face verification
        result = DeepFace.verify(img, img_to_verify, model_name="Facenet", enforce_detection=False, detector_backend="retinaface")

        # Display the result
        print(f"Verification result for {os.path.basename(person_dir)}: {result['verified']}")

        # If the person is verified, store the rectangle and label
        if result['verified']:
            identity = f"{os.path.basename(person_dir)} ({result['verified']})"
            rect = patches.Rectangle((result['facial_areas']['img1']['x'], result['facial_areas']['img1']['y']),
                                     result['facial_areas']['img1']['w'], result['facial_areas']['img1']['h'],
                                     linewidth=2, edgecolor='r', facecolor='none')
            rectangles.append(rect)
            labels.append(identity)

# Visualize the final result
fig, ax = plt.subplots(figsize=(12, 8))
ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))

# Add accumulated rectangles and labels to the image
for rect, label in zip(rectangles, labels):
    ax.add_patch(rect)
    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_y() + rect.get_height() / 2, label,
            color='r', ha='center', va='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))

ax.axis('off')
plt.show()

"""##Face Recognition for one individual Person (image)"""

from deepface import DeepFace
import os
import cv2
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# Load the image with lots of faces
img = ins_get_image('t1')

# Path to the directory containing images of different people
base_path = "friends/Train/Ross"  # Adjusted path for Ross

# Initialize lists to store rectangles and labels
rectangles = []
labels = []

# Iterate through each image in Ross's directory
for img_path in os.listdir(base_path):
    img_path = os.path.join(base_path, img_path)

    # Load the image to verify
    img_to_verify = cv2.imread(img_path)

    # Perform face verification
    result = DeepFace.verify(img, img_to_verify, model_name="Facenet", enforce_detection=False, detector_backend="retinaface")

    # Display the result
    print(f"Verification result for Ross: {result['verified']}")

    # If the verification result is true for Ross, store the rectangle and label
    if result['verified']:
        identity = f"Ross ({result['verified']})"
        rect = patches.Rectangle((result['facial_areas']['img1']['x'], result['facial_areas']['img1']['y']),
                                 result['facial_areas']['img1']['w'], result['facial_areas']['img1']['h'],
                                 linewidth=2, edgecolor='r', facecolor='none')
        rectangles.append(rect)
        labels.append(identity)

        # Visualize the frame with the added rectangle in real-time
        fig, ax = plt.subplots(figsize=(12, 8))
        ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))

        # Add accumulated rectangles and labels to the image
        for rect, label in zip(rectangles, labels):
            ax.add_patch(rect)
            ax.text(rect.get_x() + rect.get_width() / 2, rect.get_y() + rect.get_height() / 2, label,
                    color='r', ha='center', va='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))

        ax.axis('off')
        plt.pause(0.1)  # Pause to display the frame

# Close the figure at the end
plt.close()

"""##Face Recognition in video"""

from deepface import DeepFace
import cv2
import imageio
import os
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import imageio
from IPython.display import HTML, display


# Path to the directory containing images of Ross
ross_base_path = "/content/drive/MyDrive/Colab Notebooks/Deepfake/friends/Train/Ross"
# Load images of Ross
ross_images = [cv2.imread(os.path.join(ross_base_path, img_path)) for img_path in os.listdir(ross_base_path)]

# Specify the path to your input video
video_path_input = '/content/drive/MyDrive/Colab Notebooks/Deepfake/ROSS TEST.mp4'  # Replace with the actual path

# Specify the path for the output video
video_path_output = '/content/drive/MyDrive/Colab Notebooks/Deepfake/ROSS swapping TESTt.mp4'
# Create a VideoCapture object
cap = cv2.VideoCapture(video_path_input)

# Get video properties for output video
width = int(cap.get(3))
height = int(cap.get(4))
fps = int(cap.get(5))

# Create VideoWriter object
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(video_path_output, fourcc, fps, (width, height))

counter = 0
skip_frames = 3  # Take 1 out of 5 samples

# Process frames
while (cap.isOpened()):
    ret, frame = cap.read()

    if not ret:
        break

    counter += 1

    # Skip frames based on the skip_frames value
    if counter % skip_frames != 0:
        continue

    # Initialize lists to store rectangles and labels
    rectangles = []
    labels = []

    # Perform face verification for each image of Ross
    for img_to_verify in ross_images:
        # Perform face verification
        result = DeepFace.verify(frame, img_to_verify, model_name="Facenet", enforce_detection=False, detector_backend="retinaface")

        # Display the result
        print(f"Verification result for Ross: {result['verified']}")

        # If the verification result is true for Ross, store the rectangle and label
        if result['verified']:
            rect = patches.Rectangle((result['facial_areas']['img1']['x'], result['facial_areas']['img1']['y']),
                                     result['facial_areas']['img1']['w'], result['facial_areas']['img1']['h'],
                                     linewidth=2, edgecolor='r', facecolor='none')
            rectangles.append(rect)

            # Get the label (identity) for Ross
            identity = f"Ross"
            label_position = (int(rect.get_x()), int(rect.get_y()) - 10)
            cv2.putText(frame, identity, label_position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2, cv2.LINE_AA)
            break

    # Add accumulated rectangles to the frame
    for rect in rectangles:
        cv2.rectangle(frame, (int(rect.get_x()), int(rect.get_y())),
                      (int(rect.get_x() + rect.get_width()), int(rect.get_y() + rect.get_height())),
                      (0, 0, 255), 2)

    # Write the frame to the output video
    out.write(frame)

# Release resources
cap.release()
out.release()
cv2.destroyAllWindows()

"""#VIDEO SWAPPING FOR SPECIFIC CHARACTER (INSIGHT FACE & DEEP FACE)"""

from deepface import DeepFace
import cv2
import imageio
import os
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import imageio
from IPython.display import HTML, display
import dlib
import insightface
from insightface.app import FaceAnalysis
from insightface.data import get_image as ins_get_image


swapper = insightface.model_zoo.get_model('/content/drive/MyDrive/Colab Notebooks/Deepfake/inswapper_128.onnx',
                                          download=False,
                                          download_zip=False)




# Load the video
video_path_input = '/content/drive/MyDrive/Colab Notebooks/Deepfake/Ross and Rachel Argue About Baby Names _ Friends new.mp4'
cap = cv2.VideoCapture(video_path_input)

# Load images for swapping (make sure the image size is compatible with FaceSwapper)
ross_base_path = "/content/drive/MyDrive/Colab Notebooks/Deepfake/friends/Train/Ross"
ross_images = [cv2.imread(os.path.join(ross_base_path, img_path)) for img_path in os.listdir(ross_base_path)]

# Initialize dlib's face detector and facial landmarks predictor
#detector = dlib.get_frontal_face_detector()
#predictor_path = 's/content/drive/MyDrive/Colab Notebooks/Deepfake/shape_predictor_68_face_landmarks.dat'  # You need to download this file
#predictor = dlib.shape_predictor(predictor_path)

video_path_output = '/content/drive/MyDrive/Colab Notebooks/Deepfake/Ross and Rachel Argue About Baby Names _ Friends new OUTPUT 3.mp4'


# Initialize FaceSwapper
# Detect swapping image
swapping_image = cv2.imread("/content/drive/MyDrive/Colab Notebooks/Deepfake/elon mask 2.jpg")
swapping_image_face = app.get(swapping_image)


# Get video properties for output video
width = int(cap.get(3))
height = int(cap.get(4))
fps = int(cap.get(5))

# Create VideoWriter object
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(video_path_output, fourcc, fps, (width, height))

counter = 0
skip_frames = 3  # Take 1 out of 3 samples
# Process frames
while (cap.isOpened()):
    ret, frame = cap.read()

    if not ret:
        break

    counter += 1

    # Skip frames based on the skip_frames value
    if counter % skip_frames != 0:
        continue

    # Initialize lists to store rectangles and labels
    rectangles = []
    labels = []

    # Perform face verification for each image of Ross
    for img_to_verify in ross_images:
        # Perform face verification
        result = DeepFace.verify(frame, img_to_verify, model_name="Facenet", enforce_detection=False, detector_backend="retinaface")

        expansion_factor = 3
        # Display the result
        print(f"Verification result for Ross: {result['verified']}")

        if result['verified']:
            x, y, w, h = result['facial_areas']['img1']['x'], result['facial_areas']['img1']['y'], result['facial_areas']['img1']['w'], result['facial_areas']['img1']['h']

            # Expand the region by multiplying width and height by the expansion factor
            expanded_w = int(w * expansion_factor)
            expanded_h = int(h * expansion_factor)

            # Ensure that the expanded region is within the frame boundaries
            x_expanded = max(0, x - (expanded_w - w) // 2)
            y_expanded = max(0, y - (expanded_h - h) // 2)

            # Extract the face from the expanded area
            small_frame=frame[y_expanded:y_expanded+expanded_h, x_expanded:x_expanded+expanded_w]
            detected_face = app.get(small_frame)

            if detected_face:
              # Swap faces using the swapper model
              swapping_result = swapper.get(small_frame, detected_face[0], swapping_image_face[0], paste_back=True)

              # Replace the face in the frame with the swapping result
              frame[y_expanded:y_expanded+expanded_h, x_expanded:x_expanded+expanded_w] = swapping_result

              #plt.imshow(frame[:, :, ::-1])  # Display the modified frame
              #plt.show()

            break




    # Write the frame to the output video
    out.write(frame)

# Release resources
cap.release()
out.release()
cv2.destroyAllWindows()